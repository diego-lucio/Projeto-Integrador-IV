# Pipeline de Dados ‚Äì Qualidade do Ar (Votorantim/Sorocaba)

[![Python](https://img.shields.io/badge/Python-3.11-blue)]() [![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)]() [![Made with ‚ù§Ô∏è in SP](https://img.shields.io/badge/Made%20in-S√£o%20Paulo-red)]()

Automatiza a ingest√£o de **CETESB (qualidade do ar)** e **INMET (meteorologia)** para um **MySQL**, preparando a base para an√°lises, ML e dashboards.

## üì¶ Conte√∫do
- `sql/db_schema.sql` ‚Äî Cria√ß√£o do banco e tabelas.
- `scripts/db.py` ‚Äî Helper de conex√£o via SQLAlchemy.
- `scripts/etl_cetesb.py` ‚Äî Importa CSV da CETESB (ou fallback OpenAQ).
- `scripts/etl_inmet.py` ‚Äî Importa dados hor√°rios do INMET (API).
- `pipeline.py` ‚Äî Orquestra a execu√ß√£o dos dois ETLs.
- `config/.env.example` ‚Äî Exemplo de configura√ß√£o (.env).
- `requirements.txt` ‚Äî Depend√™ncias Python.

## ‚öôÔ∏è Pr√©-requisitos
- Python 3.10+
- MySQL 8+
- Criar um `.env` a partir de `config/.env.example` (na raiz do projeto).

## üóÑÔ∏è Banco de Dados
1. Suba o MySQL e crie o schema:
```sql
SOURCE sql/db_schema.sql;
```
2. Confirme usu√°rio/senha e defina `DB_URL` no `.env`, por exemplo:
```env
DB_URL=mysql+pymysql://root:root@localhost:3306/air_quality?charset=utf8mb4
```

## üß™ Ambiente Python
```bash
python -m venv .venv
source .venv/bin/activate   # Windows: .venv\Scripts\activate
pip install -r requirements.txt
cp config/.env.example .env
# edite .env conforme necess√°rio
```

## üì• CETESB ‚Äì Como obter o CSV
1. Acesse **https://qualar.cetesb.sp.gov.br/qualar/home.do**.
2. Menu **Consulta ‚Üí Dados Hor√°rios**.
3. Selecione **Esta√ß√£o Sorocaba ‚Äì Parque Vit√≥ria R√©gia** e os poluentes desejados (ex.: PM2.5, PM10, CO, NO2).
4. Exporte **CSV** e salve (ex.: `./data/cetesb_sorocaba_2024.csv`).
5. No `.env`, ajuste:
```env
CETESB_CSV_PATH=./data/cetesb_sorocaba_2024.csv
CETESB_STATION_CODE=29053002
CETESB_STATION_NAME=Sorocaba - Parque Vit√≥ria R√©gia
```
> Se voc√™ tiver um **URL direto** para o CSV, use `CETESB_CSV_URL`. Se nenhum for definido, o script tenta **OpenAQ (Sorocaba)** como fallback.

## üå¶Ô∏è INMET ‚Äì API
Edite no `.env`:
```env
INMET_STATION=A703
INMET_START_DATE=2024-01-01
INMET_END_DATE=2024-12-31
```

## ‚ñ∂Ô∏è Executar o Pipeline
```bash
python pipeline.py
```
Isso vai:
1) carregar a CETESB (CSV local/URL ou OpenAQ) ‚Üí `cetesb_readings`
2) carregar INMET (API) ‚Üí `inmet_readings`

## üß© Dicas de Uso
- Chave prim√°ria evita duplicidades (UPSERT).
- Depois de populado, consulte a *view* `vw_air_quality_join` para an√°lises r√°pidas.
- Para dashboards (Power BI/Metabase), a conex√£o no MySQL j√° estar√° pronta.

## üõ†Ô∏è Troubleshooting
- **Erro de conex√£o DB_URL**: confira usu√°rio/senha/host/porta no `.env`.
- **CSV CETESB com layout diferente**: ajuste parsing em `scripts/etl_cetesb.py` (_read_cetesb_csv).
- **Sem dados OpenAQ**: baixe manualmente o CSV da CETESB e aponte `CETESB_CSV_PATH`.


---

## üê≥ Docker Compose (MySQL + Adminer + Metabase)
1. Copie `.env.docker` para `.env` (ou exporte as vari√°veis no shell):
```bash
cp .env.docker .env
```
2. Suba a stack:
```bash
docker compose up -d
```
3. Acesse:
- Adminer: http://localhost:8080  (Server: `mysql`, User: `${MYSQL_USER}` ou `root`)
- Metabase: http://localhost:3000  (configure a conex√£o MySQL `aq_mysql`)

## üìä EDA inicial (gr√°ficos e CSVs)
Com o banco j√° populado, rode:
```bash
python scripts/eda_initial.py
```
Sa√≠das ficam em `./outputs`:
- `00_counts.csv` ‚Äî contagem de registros por tabela
- `01_date_ranges.csv` ‚Äî intervalo de datas
- `02_pollutant_overview.csv` e `02a_pollutant_top10_count.png`
- `03_timeseries_hourly.png` ‚Äî s√©rie por hora do poluente-alvo (padr√£o: pm2_5)
- `04_corr_pm25_temp.txt` e `04_scatter_pm25_vs_temp.png`

Para trocar o poluente, defina `EDA_POLLUTANT` no `.env`.


---

## üöÄ ETL em Container (servi√ßo no docker-compose)
O `docker-compose.yml` agora inclui um servi√ßo **etl** que constr√≥i a imagem a partir de `Dockerfile.etl`, instala depend√™ncias e executa `pipeline.py` automaticamente.

### Subir tudo (DB + Adminer + Metabase + ETL)
```bash
cp .env.docker .env          # ou exporte as vari√°veis do compose
docker compose up -d --build  # constr√≥i a imagem do ETL e sobe a stack
```

### Execu√ß√£o peri√≥dica
- Por padr√£o, o ETL roda **a cada 60 min** (vari√°vel `ETL_INTERVAL_MINUTES`).
- Para rodar **uma √∫nica vez**, remova `ETL_INTERVAL_MINUTES` do servi√ßo `etl` no compose.

### Logs do ETL
```bash
docker compose logs -f etl
```

## üß∞ Publicar no GitHub
1. Crie um novo reposit√≥rio no GitHub.
2. Execute no diret√≥rio do projeto:
```bash
git init
git add .
git commit -m "feat: ETL + Docker + EDA inicial"
git branch -M main
git remote add origin https://github.com/<usuario>/<repositorio>.git
git push -u origin main
```


---

## ‚úÖ GitHub Actions (CI)
Workflow em `.github/workflows/ci.yml`:
- Sobe MySQL 8 como **service**
- Instala depend√™ncias
- Aplica `sql/db_schema.sql`
- Executa `pipeline.py` (usa OpenAQ como fallback se n√£o houver CSV)
- Roda `scripts/eda_initial.py`
- Faz `docker build` do ETL

### Rodando localmente com Makefile
```bash
make install
make etl
make eda
```

## üìä Metabase ‚Äì Template
Importe `metabase/starter_dashboard.json` (Serialization) e ajuste as fontes. Veja `metabase/README.md`.
